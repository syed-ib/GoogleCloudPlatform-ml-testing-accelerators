# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"apiVersion": "batch/v1"
"kind": "CronJob"
"metadata":
  "labels":
    "accelerator": "v3-8"
    "benchmarkId": "pt-r1.13-dlrm-conv-v3-8"
    "frameworkVersion": "pt-r1.13"
    "mode": "conv"
    "model": "dlrm"
  "name": "pt-r1.13-dlrm-conv-v3-8"
  "namespace": "automated"
"spec":
  "concurrencyPolicy": "Forbid"
  "jobTemplate":
    "metadata":
      "annotations":
        "ml-testing-accelerators/gcs-subdir": "pt-r1.13/dlrm/conv/v3-8"
        "ml-testing-accelerators/metric-config": |
          {
            "sources": [
              {
                "literals": {
                  "assertions": {
                    "duration": {
                      "inclusive_bounds": false,
                      "std_devs_from_mean": {
                        "comparison": "LESS",
                        "std_devs": 5
                      },
                      "wait_for_n_data_points": 10
                    }
                  }
                }
              },
              {
                "tensorboard": {
                  "aggregate_assertions": [
                    {
                      "assertion": {
                        "inclusive_bounds": false,
                        "std_devs_from_mean": {
                          "comparison": "LESS",
                          "std_devs": 5
                        },
                        "wait_for_n_data_points": 10
                      },
                      "strategy": "FINAL",
                      "tag": "ExecuteTime__Percentile_99_sec"
                    },
                    {
                      "assertion": {
                        "inclusive_bounds": true,
                        "std_devs_from_mean": {
                          "comparison": "LESS",
                          "std_devs": 0
                        },
                        "wait_for_n_data_points": 0
                      },
                      "strategy": "FINAL",
                      "tag": "aten_ops_sum"
                    }
                  ],
                  "exclude_tags": [
                    "LearningRate"
                  ],
                  "include_tags": [
                    {
                      "strategies": [
                        "FINAL"
                      ],
                      "tag_pattern": "*"
                    }
                  ],
                  "merge_runs": true
                }
              }
            ]
          }
      "labels":
        "accelerator": "v3-8"
        "benchmarkId": "pt-r1.13-dlrm-conv-v3-8"
        "frameworkVersion": "pt-r1.13"
        "mode": "conv"
        "model": "dlrm"
    "spec":
      "activeDeadlineSeconds": 21600
      "backoffLimit": 1
      "template":
        "metadata":
          "annotations":
            "reserved.cloud-tpus.google.com": "false"
            "tf-version.cloud-tpus.google.com": "pytorch-1.13"
        "spec":
          "containers":
          - "env":
            - "name": "POD_NAME"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.name"
            - "name": "POD_NAMESPACE"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.namespace"
            "image": "gcr.io/xl-ml-test/health-monitor:stable"
            "imagePullPolicy": "Always"
            "name": "monitor"
          - "args":
            - "/bin/bash"
            - "-c"
            - |
              set -u
              set -e
              set -x
              
              set +e
              apt-get install -y bc
              pip install onnx
              git clone --recursive https://github.com/pytorch-tpu/examples.git
              'python3' \
              'tpu-examples/deps/dlrm/dlrm_tpu_runner.py' \
              '--arch-interaction-op=dot' \
              '--lr-num-warmup-steps=10' \
              '--lr-decay-start-step=10' \
              '--num-batches=1000' \
              '--data-generation=dataset' \
              '--numpy-rand-seed=72' \
              '--print-freq=100' \
              '--use-tpu' \
              '--metrics-debug' \
              '--num-indices-per-lookup-fixed' \
              '--mini-batch-size=128' \
              '--arch-embedding-size=1000000-1000000' \
              '--tpu-model-parallel-group-len=8' \
              '--tpu-cores=8' \
              '--raw-data-file=/datasets/criteo-kaggle-mm/train.txt' \
              '--processed-data-file=/datasets/criteo-kaggle-mm/kaggleAdDisplayChallenge_processed.npz' \
              '--memory-map' \
              '--print-time' \
              '--test-mini-batch-size=16384' \
              '--test-freq=101376' \
              '--data-set=kaggle' \
              '--loss-function=bce' \
              '--round-targets=True' \
              '--learning-rate=0.1' \
              '--print-freq=1024' \
              '--no-save' \
              '--nepochs=25'
              
            "command":
            - "bash"
            - "-cxue"
            - |
              if [[ ! -z "$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)" ]]; then
                # Trim grpc:// prefix
                export XRT_TPU_CONFIG="tpu_worker;0;${KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS:7}"
              fi
              
              # Run whatever is in `command` here
              docker-entrypoint.sh "${@:0}"
            "env":
            - "name": "POD_NAME"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.name"
            - "name": "POD_UID"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.uid"
            - "name": "POD_NAMESPACE"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.namespace"
            - "name": "JOB_NAME"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.labels['job-name']"
            - "name": "MODEL_DIR"
              "value": "$(OUTPUT_BUCKET)/pt-r1.13/dlrm/conv/v3-8/$(JOB_NAME)"
            - "name": "XLA_USE_BF16"
              "value": "0"
            "envFrom":
            - "configMapRef":
                "name": "gcs-buckets"
            - "configMapRef":
                "name": "pytorch-nfs-ip"
            "image": "gcr.io/tpu-pytorch/xla:r1.13_3.7"
            "imagePullPolicy": "Always"
            "name": "train"
            "resources":
              "limits":
                "cloud-tpus.google.com/v3": 8
                "tpu.googleapis.com/v3": 8
              "requests":
                "cpu": "9.0"
                "memory": "30Gi"
            "volumeMounts":
            - "mountPath": "/dev/shm"
              "name": "dshm"
              "readOnly": false
            - "mountPath": "/datasets"
              "name": "pytorch-datasets-claim"
              "readOnly": true
          "initContainers":
          - "command":
            - "python3"
            - "-c"
            - |
              import importlib_metadata
              import os
              import re
              
              import cloud_tpu_client
              
              requirements = importlib_metadata.requires('torch_xla')
              libtpu_pattern = r'libtpu-nightly ?@ https:\/\/storage.googleapis.com\/cloud-tpu-tpuvm-artifacts\/wheels\/libtpu-nightly\/libtpu_nightly-\d.\d.dev(\d{8})-\w+-\w+-\w+.whl'
              libtpu_matches = [
                re.findall(libtpu_pattern, req)[0]
                for req in requirements
                if re.match(libtpu_pattern, req)
              ]
              assert len(libtpu_matches) == 1, f'{len(libtpu_matches)} matches in {requirements} (pattern: `{libtpu_pattern}`)'
              libtpu_date = libtpu_matches[0]
              print('libtpu date:', libtpu_date)
              
              ctc = cloud_tpu_client.Client(tpu=os.path.basename('$(TPU_NAME)'), zone=os.path.dirname('$(TPU_NAME)'))
              ctc.wait_for_healthy()
              ctc.configure_tpu_version(f'pytorch-nightly-dev{libtpu_date}', restart_type='always')
              ctc.wait_for_healthy()
            "env":
            - "name": "TPU_NAME"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.annotations['name.cloud-tpus.google.com/train']"
            "image": "gcr.io/tpu-pytorch/xla:r1.13_3.7"
            "name": "tpu-version"
          "nodeSelector":
            "tpu-available": "true"
          "priorityClassName": "tpu-device"
          "restartPolicy": "Never"
          "volumes":
          - "emptyDir":
              "medium": "Memory"
            "name": "dshm"
          - "name": "pytorch-datasets-claim"
            "persistentVolumeClaim":
              "claimName": "pytorch-datasets-claim"
  "schedule": "0 6 * * 0,5"
  "successfulJobsHistoryLimit": 1